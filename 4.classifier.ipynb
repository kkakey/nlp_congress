{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "with open(\"./pickled_data/all_dfs.dat\", \"rb\") as f:\n",
    "    all_dfs = pickle.load(f)\n",
    "    \n",
    "#unpack all_dfs\n",
    "df_final, df_95, df_96, df_97, df_98, df_99, df_100, df_101, df_102, df_103,df_104, df_105, df_106, df_107, df_108, df_109, df_110,  df_111 = all_dfs\n",
    "\n",
    "all_dfs_complete = all_dfs[0]\n",
    "all_dfs = all_dfs[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class crawler_sol(object):\n",
    "    def grid_search_func(self, param_grid, the_mode_in, the_vec_in, the_lab_in):\n",
    "        from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "        grid_search = GridSearchCV(the_mode_in, param_grid=param_grid, cv=5)\n",
    "        best_model = grid_search.fit(the_vec_in, the_lab_in)\n",
    "        max_score = grid_search.best_score_\n",
    "        best_params = grid_search.best_params_\n",
    "\n",
    "        return best_model, max_score, best_params\n",
    "\n",
    "    def model_tune(self, xform_in, label_in, param_grid_in, path_in, df):\n",
    "        from sklearn.svm import SVC\n",
    "        import pickle\n",
    "        \n",
    "        congress = df['congress'].unique()[0]\n",
    "\n",
    "        clf_pca = SVC(probability = True)\n",
    "\n",
    "        gridsearch_model, best, opt_params = self.grid_search_func(\n",
    "        param_grid_in, clf_pca, xform_in, label_in)\n",
    "\n",
    "        print (\"Best CV performance: \" + str(best))\n",
    "        print (opt_params)\n",
    "\n",
    "        #if you want more than just accuracy (recall, F1, etc.) --- do here\n",
    "\n",
    "        clf_pca_out = SVC(probability = True)\n",
    "        clf_pca_out.set_params(**gridsearch_model.best_params_)\n",
    "        clf_pca_out.fit(xform_in, label_in)\n",
    "\n",
    "        pickle.dump(clf_pca_out, open(path_in + str(congress) + 'model.pkl', 'wb'))        \n",
    "\n",
    "        return clf_pca_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier(df):\n",
    "    import pickle\n",
    "    import pandas as pd\n",
    "    import numpy as np    \n",
    "    from imblearn.combine import SMOTETomek\n",
    "    from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "    congress = df['congress'].unique()[0]\n",
    "    print(congress)\n",
    "    \n",
    "    df = df[(df.speech_length >= np.percentile(df.speech_length, 20)) & (df.speech_length <= np.percentile(df.speech_length, 80))]\n",
    "    \n",
    "    #Define label column\n",
    "    label_col = 'ideo_vec_ext'\n",
    "    \n",
    "    df = df[df[label_col]!=\"NA\"]\n",
    "    df[label_col] = df[label_col].astype(int) \n",
    "    #comment out to include moderates (only relevant for ideo_vec and ideo_vec_ext cols)\n",
    "    df = df[df[label_col]!=0] \n",
    "\n",
    "    #Define text body column\n",
    "    body_text = np.array(df['body_lem'])\n",
    "    label = np.array(df[label_col])\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "#     #Define body_col and label to classify with\n",
    "#     body_text = np.array(df['body_lem'])\n",
    "#     label = np.array(df['ideo_vec_ext'])\n",
    "      \n",
    "    \n",
    "#     df = df[(df.speech_length >= np.percentile(df.speech_length, 20)) & (df.speech_length <= np.percentile(df.speech_length, 80))]\n",
    "#     #Define label\n",
    "#     label = np.array(df['ideo_vec_ext'])\n",
    "#     df[label] = df[label].astype(int) \n",
    "#     #comment out to include moderates (only relevant for ideo_vec and ideo_vec_ext cols)\n",
    "#     df = df[df[label]!=0] \n",
    "    \n",
    "#     body_text = np.array(df['body_lem'])\n",
    "    #    df = df[df[label]!=\"NA\"]\n",
    "    \n",
    "#     df[label] = df[label].astype(int) \n",
    "#     #comment out to include moderates (only relevant for ideo_vec and ideo_vec_ext cols)\n",
    "#     df = df[df[label]!=0] \n",
    "    \n",
    "    \n",
    "    sme = SMOTETomek(random_state=42,ratio='minority')\n",
    "\n",
    "    my_vec_tfidf_out = TfidfVectorizer()\n",
    "    my_xform_tfidf_out = my_vec_tfidf_out.fit_transform(body_text)\n",
    "    my_pd = pd.DataFrame(my_xform_tfidf_out.toarray())\n",
    "    my_pd.columns = my_vec_tfidf_out.get_feature_names()\n",
    "    X = my_pd.values\n",
    "\n",
    "    print(pd.Series(label).value_counts())\n",
    "\n",
    "    #balance data\n",
    "    X_res, y_res = sme.fit_sample(X, label) \n",
    "    print(pd.Series(y_res).value_counts())\n",
    "\n",
    "\n",
    "    my_pd = pd.DataFrame(X_res)\n",
    "    my_pd.columns = my_vec_tfidf_out.get_feature_names()\n",
    "    X_res = my_pd.values\n",
    "    \n",
    "    path_in = './vecs+pca/'\n",
    "    pickle.dump(my_vec_tfidf_out, open(path_in + str(congress) + 'tfidfvectorizer.pkl', 'wb'))\n",
    "\n",
    "    print('start svd')\n",
    "    from sklearn import decomposition\n",
    "    from sklearn.decomposition import TruncatedSVD\n",
    "    n_comp = 1\n",
    "    var_fig = 0.0\n",
    "    while var_fig <= 0.95:\n",
    "        svd = decomposition.TruncatedSVD(n_components=n_comp*10, algorithm='arpack')\n",
    "        my_dim = svd.fit_transform(X_res)\n",
    "        var_fig = svd.explained_variance_ratio_.sum()\n",
    "        print(var_fig)\n",
    "        n_comp += 50\n",
    "\n",
    "    my_func = crawler_sol()\n",
    "\n",
    "    #grid params\n",
    "    print('start grim params')\n",
    "    param_grid = {\"gamma\": [\"scale\", \"auto\"],\n",
    "                  \"decision_function_shape\": [\"ovo\", \"ovr\"],\n",
    "                 \"C\": [1.0,2.0]}\n",
    "\n",
    "    the_path=\"./model/\"\n",
    "\n",
    "    #model tuning, train, text, validation\n",
    "    the_model = my_func.model_tune(\n",
    "            my_dim, y_res, param_grid, the_path, df)\n",
    "\n",
    "\n",
    "    path_in = './vecs+pca/'\n",
    "\n",
    "    congress = df['congress'].unique()[0]\n",
    "\n",
    "    pickle.dump(svd, open(path_in + str(congress) + 'svd.pkl', 'wb'))\n",
    "    print('finish')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# for i, df in enumerate(all_dfs):\n",
    "#     classifier(all_dfs[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "111\n",
      "-1    1202\n",
      " 1     505\n",
      "dtype: int64\n",
      " 1    1200\n",
      "-1    1200\n",
      "dtype: int64\n",
      "start svd\n",
      "0.2261407380870616\n",
      "0.8181537134835816\n",
      "0.9636605223628232\n",
      "start grim params\n",
      "Best CV performance: 0.8704166666666666\n",
      "{'C': 2.0, 'decision_function_shape': 'ovo', 'gamma': 'scale'}\n",
      "finish\n"
     ]
    }
   ],
   "source": [
    "classifier(df_111)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
